# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13l_HfwohHkkJlTr-FbAtLLVJR7CjyGdc
"""

from google.colab import files 
  
  
uploaded = files.upload()

"""**Define a polynomial regression model for predicting the house price and test the model against test set.**

**Importing all the libraries**.
"""

#for reading the data
import pandas as pd

#for data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#for making linear regression model
from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

"""**For Reading the data**"""

import pandas as pd 
import io 
  
df = pd.read_csv(io.BytesIO(uploaded['train.csv'])) 
print(df)

import io 
df_test = pd.read_excel(io.BytesIO(uploaded['test.xlsx']))
print(df_test)

"""**Name of columns and description of data**"""

# Commented out IPython magic to ensure Python compatibility.
import io 

# %matplotlib inline
#bring in the six packs
df_train =pd.read_csv(io.BytesIO(uploaded['train.csv'])) 

#check the decoration
df_train.columns

#descriptive statistics summary
df_train['price'].describe()
#analysing sale price first

"""**To get information regarding the datatypes.**"""

print(df_train.info())

"""**To check for duplicate or null values as a part of data preprocessing.**"""

dupl_rows = df_train.duplicated().sum()
print(dupl_rows)
null_v = df_train.isnull().sum()
print(null_v)

"""**From the above descriptive analysis we can say the data does not require any
preprocessing.**

# **Visualization of data**

**Creating a distplot for the price values to see if the data is skewed in any manner and whether it might need some normalizing or
not.**
"""

#histogram
sns.distplot(df_train['price']);
#plotting histogram

sns.distplot(df_train['price'], fit = norm)

sns.distplot(np.log(df_train['price']), fit = norm)

#skewness and kurtosis
print("Skewness: %f" % df_train['price'].skew())
print("Kurtosis: %f" % df_train['price'].kurt())

"""**To select the important features for predicting the price on the basis of the heatmap.**"""

#correlation matrix
corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

#price correlation matrix
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'price')['price'].index
cm = np.corrcoef(df_train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

#scatterplot
sns.set()
cols = ['price', 'id', 'date', 'bedrooms', 'bathrooms', 'sqft_living',
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
       'lat', 'long', 'sqft_living15', 'sqft_lot15']
# here i ve chosen the 10 most important features for prediciting house prices
sns.pairplot(df_train[cols], size = 2.5)
plt.show();

"""**Extracting the important features**"""

X = df_train["sqft_living"]
y = df_train["price"] 
plt.scatter(X,y)
plt.show()

X_feat = [imp_feat.index]
X1 =[]
for i in range(0,11):
  X1.append(X_feat[0][i])
print(X1)

target = abs(corr["price"])
imp_feat = target[target > 0.1]
print(imp_feat)

"""**Visualizing some of the important features with respect to the target variable**"""

cor_mat = df_train.corr()
feat = 11
fields = cor_mat.nlargest(feat, 'price')['price'].index
field = list(fields)
field.remove('price')
print(field)

X = df_train["sqft_living"]
y = df_train["price"]
plt.scatter(X,y)
plt.show()

X = df_train["grade"]
plt.scatter(X,y)
plt.show()

X = df_train["sqft_above"]
plt.scatter(X,y)
plt.show()

X = df_train["sqft_living15"]
plt.scatter(X,y)
plt.show()

X = df_train["bathrooms"]
plt.scatter(X,y)
plt.show()

"""**We build a polynomial regression model now**

For degree 4
"""

polyF = PolynomialFeatures(degree = 4)
x_poly_train = polyF.fit_transform(df_train[X1])
x_poly_test = polyF.fit_transform(df_test[X1])
LinReg = LinearRegression()
LinReg.fit(x_poly_train, df_train['price'])
yPred = LinReg.predict(x_poly_test)

"""Since we're performing regression, we use MSE and RMSE to evaluate the models, with lower values for
each indicating a more accurate model.
"""

from sklearn import metrics
print("Degree 4:")
MSE = metrics.mean_squared_error(df_test['price'], yPred)
print("MSE:", round(np.sqrt(MSE),2))
print("R-squared train: ", round(LinReg.score(x_poly_train, df_train['price']),3))
print("R-squared test: ", round(LinReg.score(x_poly_test, df_test['price']),3))

"""For degree 3"""

polyF = PolynomialFeatures(degree = 3)
x_poly_train = polyF.fit_transform(df_train[X1])
x_poly_test = polyF.fit_transform(df_test[X1])
LinReg = LinearRegression()
LinReg.fit(x_poly_train, df_train['price'])
yPred = LinReg.predict(x_poly_test)

print("Degree 3:")
MSE = metrics.mean_squared_error(df_test['price'], yPred)
print("MSE:", round(np.sqrt(MSE),2))
print("R-squared train: ", round(LinReg.score(x_poly_train, df_train['price']),3))
print("R-squared test: ", round(LinReg.score(x_poly_test, df_test['price']),3))

"""For degree 2"""

polyF = PolynomialFeatures(degree = 2)
x_poly_train = polyF.fit_transform(df_train[X1])
x_poly_test = polyF.fit_transform(df_test[X1])
LinReg = LinearRegression()
LinReg.fit(x_poly_train, df_train['price'])
yPred = LinReg.predict(x_poly_test)
print("Degree 2:")
MSE = metrics.mean_squared_error(df_test['price'], yPred)
print("MSE:", round(np.sqrt(MSE),2))
print("R-squared train: ", round(LinReg.score(x_poly_train, df_train['price']),3))
print("R-squared test: ", round(LinReg.score(x_poly_test, df_test['price']),3))

"""**Conclusion**

From the analysis and prediction performed on the training and test datasets, it can conclusively be
stated that the best Polynomial Regressor for the given problem is having a degree = 3. It is also slightly
better than performing simple multivariate linear regression (so is polynomial degree = 2)as is evident
from the slight improvement in the values of MSE and RMSE.
"""